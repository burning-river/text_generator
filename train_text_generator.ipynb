{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Px_1TDSfN9Vm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch import cuda, tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from huggingface_hub import HfFolder, Repository, create_repo\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vp-ZD7QylsxX"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlyuMKwdmGZx"
   },
   "outputs": [],
   "source": [
    "cwd = '/content/drive/MyDrive/LLMs'\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yjvdEzqnZXd"
   },
   "outputs": [],
   "source": [
    "filename = f'{cwd}/subreddits_train_data.parquet'\n",
    "raw_df = pd.read_parquet(filename)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOgcVhKNxU8t"
   },
   "outputs": [],
   "source": [
    "raw_df['PostText'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYrJ4s41AMJM"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvUMYspWAEkF"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztJe_J5fIYon"
   },
   "outputs": [],
   "source": [
    "def shuffle_text(text):\n",
    "    keywords = []\n",
    "    for item in nlp(text):\n",
    "        if not item.is_stop:\n",
    "            keywords.append(item.text)\n",
    "    keywords_para = ' '.join(keywords)\n",
    "\n",
    "    return keywords_para\n",
    "\n",
    "def extract_keywords(text):\n",
    "    try:\n",
    "        split_text = text.split(\"Text: \")\n",
    "        text = split_text[1]\n",
    "        keywords_para = shuffle_text(text)\n",
    "\n",
    "        return {\n",
    "        \"text\": text,\n",
    "        \"keywords\": keywords_para\n",
    "        }\n",
    "    except:\n",
    "        return {'text': '', 'keywords': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CT0V9QLJUgI"
   },
   "outputs": [],
   "source": [
    "keywords = []\n",
    "texts = []\n",
    "for post in raw_df['PostText'].values:\n",
    "    result = extract_keywords(post)\n",
    "    keywords.append(result['keywords'])\n",
    "    texts.append(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFkg8kScGec6"
   },
   "outputs": [],
   "source": [
    "raw_df['PostText'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDXfiDTIPRLl"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': texts, 'keywords': keywords})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpvzxjxT79r3"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fMlHPoX6Oxn"
   },
   "outputs": [],
   "source": [
    "df.to_parquet(f'{cwd}/subreddits_train_data_keywords.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0n5oeU9EkI1"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(f'{cwd}/subreddits_train_data_keywords.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlHALhky1G7N"
   },
   "outputs": [],
   "source": [
    "df['text'].values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_YfdDiY1LOV"
   },
   "outputs": [],
   "source": [
    "df['keywords'].values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrOo14bnPXLi"
   },
   "outputs": [],
   "source": [
    "X = df['keywords']\n",
    "y = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJxQaWfdPS_C"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9d-3XGamJp32",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foshhKgXPfXH"
   },
   "outputs": [],
   "source": [
    "X_train.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sghYu3fVQZ8C"
   },
   "outputs": [],
   "source": [
    "y_train.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aL89AEDlRNNl"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZulFevuNQqh"
   },
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model = model.to(device)\n",
    "max_source_length = 128\n",
    "max_target_length = 128\n",
    "task_prefix = \"generate: \"\n",
    "input_sequences = [task_prefix + text for text in X_train.values]\n",
    "encoding = tokenizer(\n",
    " input_sequences,\n",
    " padding=\"longest\",\n",
    " max_length=max_source_length,\n",
    " truncation=True,\n",
    " return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGfFcK6kJrHE"
   },
   "outputs": [],
   "source": [
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "target_encoding = tokenizer(\n",
    " list(y_train.values),\n",
    " padding=\"longest\",\n",
    " max_length=max_target_length,\n",
    " truncation=True,\n",
    " return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWDi4cXNQnOp"
   },
   "outputs": [],
   "source": [
    "labels = target_encoding.input_ids.to(device)\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "train_dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "accumulation_steps = 4\n",
    "num_epochs = 4\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Started with epoch: {epoch}')\n",
    "    for i, (input_ids_batch, attention_mask_batch, labels_batch) in enumerate(train_loader):\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attention_mask_batch = attention_mask_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids_batch, attention_mask=attention_mask_batch, labels=labels_batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss = loss/accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ec865NQTCzh"
   },
   "outputs": [],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brgDyrLkv3M1"
   },
   "outputs": [],
   "source": [
    "i = 6\n",
    "shuffled_text = X_test.values[i]\n",
    "shuffled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TX5Mi7BjLyTp"
   },
   "outputs": [],
   "source": [
    "y_test.values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-w0cpSVvuBb"
   },
   "outputs": [],
   "source": [
    "shuffled_text = f\"generate: {shuffled_text}\"\n",
    "input_ids = tokenizer(shuffled_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "output = model.generate(input_ids, max_length=400)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-y3xo2nVY6g9"
   },
   "outputs": [],
   "source": [
    "answers_fine_tuned = []\n",
    "for ind in range(10):\n",
    "    test_text = X_test.values[ind]\n",
    "    test_text = f\"generate: {test_text}\"\n",
    "    input_ids = tokenizer(test_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    output = model.generate(input_ids, max_length=50)\n",
    "    answers_fine_tuned.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "    print('Ground truth:', y_test.values[ind])\n",
    "    print('Prediction (fine tuned model):', answers_fine_tuned[ind])\n",
    "    print('Keywords:', X_test.values[ind])\n",
    "    print('\\n')\n",
    "    print('***'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8iZA8x4dnff"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8ixugt_denZ"
   },
   "outputs": [],
   "source": [
    "token = HfFolder.get_token()\n",
    "if token is None:\n",
    "  raise ValueError(\"You must be logged into the Hugging Face CLI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTAN21uUefnv"
   },
   "outputs": [],
   "source": [
    "repo_name = \"SubredditSummarizer\"\n",
    "username = 'saurabhkumar3400'\n",
    "\n",
    "repo_path = os.path.join(username, repo_name)\n",
    "\n",
    "# create_repo(repo_path, token=HfFolder.get_token(), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPEqdDeVRbTg"
   },
   "outputs": [],
   "source": [
    "repo = Repository(repo_path, clone_from=f\"{repo_path}\", use_auth_token=True)\n",
    "model.save_pretrained(repo_path)\n",
    "tokenizer.save_pretrained(repo_path)\n",
    "repo.push_to_hub(commit_message=\"sentence level, n_epochs = 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPcbsdikSHHg"
   },
   "outputs": [],
   "source": [
    "model_repo = os.path.join(username, repo_name)\n",
    "dwnld_tokenizer = T5Tokenizer.from_pretrained(model_repo)\n",
    "dwnld_model = T5ForConditionalGeneration.from_pretrained(model_repo)\n",
    "dwnld_model = dwnld_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHYvy99wI_HY"
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEEJJu3UXdpw"
   },
   "outputs": [],
   "source": [
    "answers_fine_tuned = []\n",
    "batch_size = 100\n",
    "for test_text in X_test.values[:batch_size]:\n",
    "    test_text = f\"generate: {test_text}\"\n",
    "    input_ids = dwnld_tokenizer(test_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    output = dwnld_model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "    answers_fine_tuned.append(dwnld_tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wH4R6KZ96eEH"
   },
   "outputs": [],
   "source": [
    "rouge_metric = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LVj_0Ej6ke5"
   },
   "outputs": [],
   "source": [
    "def print_rouge(rouge_n):\n",
    "\n",
    "  fine_tuned_rouge1_prec = [rouge_metric.score(y_test.values[ind], answers_fine_tuned[ind])[rouge_n].precision for ind in range(batch_size)]\n",
    "\n",
    "  fine_tuned_rouge1_recall = [rouge_metric.score(y_test.values[ind], answers_fine_tuned[ind])[rouge_n].recall for ind in range(batch_size)]\n",
    "\n",
    "  fine_tuned_rouge1_fscore = [rouge_metric.score(y_test.values[ind], answers_fine_tuned[ind])[rouge_n].fmeasure for ind in range(batch_size)]\n",
    "\n",
    "  print('Fine-tuned model (precision):', np.mean(fine_tuned_rouge1_prec))\n",
    "  print('\\n')\n",
    "  print('Fine-tuned model (recall):', np.mean(fine_tuned_rouge1_recall))\n",
    "  print('\\n')\n",
    "  print('Fine-tuned model (fscore):', np.mean(fine_tuned_rouge1_fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEbpCoW0Gw17"
   },
   "outputs": [],
   "source": [
    "print_rouge('rouge1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEWPzpqaG091"
   },
   "outputs": [],
   "source": [
    "print_rouge('rougeL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqJVggMQ7OTv"
   },
   "outputs": [],
   "source": [
    "weights = (0.25, 0.25, 0, 0)  # Weights for uni-gram, bi-gram, tri-gram, and 4-gram\n",
    "\n",
    "fine_tuned_bleu = [sentence_bleu([y_test.values[ind].split()], answers_fine_tuned[ind].split(), weights=weights, smoothing_function=SmoothingFunction().method1) for ind in range(batch_size)]\n",
    "print('Fine-tuned model:', np.mean(fine_tuned_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRxm88A6APGU"
   },
   "outputs": [],
   "source": [
    "# BERTScore calculation\n",
    "bert_scorer = BERTScorer(model_type='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Dt1tUlwAgZ_"
   },
   "outputs": [],
   "source": [
    "bert_scores = [bert_scorer.score([answers_fine_tuned[ind]], [y_test.values[ind]]) for ind in range(batch_size)]\n",
    "precision = [bert_scores[ind][0] for ind in range(batch_size)]\n",
    "recall = [bert_scores[ind][1] for ind in range(batch_size)]\n",
    "f1 = [bert_scores[ind][2] for ind in range(batch_size)]\n",
    "print('Fine-tuned model (precision):', np.mean(precision))\n",
    "print('Fine-tuned model (recall):', np.mean(recall))\n",
    "print('Fine-tuned model (fscore):', np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpWuBu5NTZ4D"
   },
   "outputs": [],
   "source": [
    "space = {\n",
    " 'learning_rate': hp.loguniform('learning_rate', -2, -1),\n",
    " 'per_device_train_batch_size': hp.choice( 'per_device_train_batch_size', [8, 16, 32, 64]),\n",
    " 'num_train_epochs': hp.choice('num_train_epochs', [1, 2, 3]),\n",
    " 'weight_decay': hp.uniform('weight_decay', 0.0, 0.3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBRwSg2hVjaL"
   },
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({'text': X_train.values, 'labels': y_train.values})\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAB7m-mHVxIc"
   },
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({'text': X_test.values, 'labels': y_test.values})\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5Q_sbRKbrSn"
   },
   "outputs": [],
   "source": [
    "class RedditDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aECItIwDcdvM"
   },
   "outputs": [],
   "source": [
    "train_encodings = dwnld_tokenizer(list(X_train.values), truncation=True, padding=True)\n",
    "test_encodings = dwnld_tokenizer(list(X_test.values), truncation=True, padding=True)\n",
    "\n",
    "train_label_encodings = dwnld_tokenizer(list(y_train.values), truncation=True, padding=True)\n",
    "test_label_encodings = dwnld_tokenizer(list(y_test.values), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-BaNw15WoY8"
   },
   "outputs": [],
   "source": [
    "train_dataset = RedditDataset(train_encodings, train_label_encodings)\n",
    "test_dataset = RedditDataset(test_encodings, test_label_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7I1p3h8TlcH"
   },
   "outputs": [],
   "source": [
    "def objective(hyperparams):\n",
    "    model_repo = os.path.join(username, repo_name)\n",
    "    dwnld_tokenizer = T5Tokenizer.from_pretrained(model_repo)\n",
    "    dwnld_model = T5ForConditionalGeneration.from_pretrained(model_repo)\n",
    "    dwnld_model = dwnld_model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_repo,\n",
    "        learning_rate=hyperparams['learning_rate'],\n",
    "        per_device_train_batch_size=hyperparams['per_device_train_batch_size'],\n",
    "        num_train_epochs=hyperparams['num_train_epochs'],\n",
    "        weight_decay=hyperparams['weight_decay'],\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=None\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=dwnld_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    return {'loss': eval_results[\"eval_loss\"], 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RERNNpd6Yi_O"
   },
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZmFWXpRYnvB"
   },
   "outputs": [],
   "source": [
    "wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F80HOw--VIeK"
   },
   "outputs": [],
   "source": [
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    max_evals=3,\n",
    "    trials=Trials()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
