{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3f6612-5fa7-4189-a4e7-612df5321a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import MapType, StringType, ArrayType, IntegerType, StructType, StructField\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e55d8e-13d2-4e00-95c5-0a711b3b8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 'reddit-tifu'\n",
    "AWS_ACCESS_KEY_ID = '***'\n",
    "AWS_SECRET_ACCESS_KEY = '***'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36dd24c3-a730-41b5-8df4-feca142b16c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark(appname, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY):\n",
    "    conf = SparkConf()\n",
    "    \n",
    "    conf.setAll([\n",
    "        (\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\"),\n",
    "        (\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID),\n",
    "        (\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
    "    ])\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName(appname) \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca29721-9077-48dd-8489-d21fbf7dc45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dc79ae75-ee42-4f74-8b10-88cb38bb3808;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 214ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-dc79ae75-ee42-4f74-8b10-88cb38bb3808\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/9ms)\n",
      "25/04/14 21:37:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark(\"data prep\", AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1857df77-7c8a-4009-b0cd-0b6c47a9b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 21:37:34 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(f\"s3a://{s3_bucket}/reddit_tifu_llm.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d59ff0-c250-42bb-a2c1-03109ea740fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Prompt                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Text: I was on Skype on my tablet as I went to the toilet IMing a friend. I don't multitask very well, so I forgot one of the most important things to do before pooping. I think the best part was when I realised and told my mate who just freaked out because I was talking to him on the John! \\nTitle: forgetting to pull my underwear down before i pooped. ####|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7184fc-4ce1-4bf9-a026-9874831aa12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(prompt):\n",
    "    split_prompt = prompt.split(\" \\nTitle: \")\n",
    "    text = split_prompt[0]\n",
    "    text = text.replace('Text: ', '')\n",
    "    title = split_prompt[1].split(\" ####\")[0]\n",
    "    return {\n",
    "    \"text\": text,\n",
    "    \"title\": title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b6c410-61d2-4eff-b54b-d12e7948bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|AnnotatedSections                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{title -> forgetting to pull my underwear down before i pooped., text -> I was on Skype on my tablet as I went to the toilet IMing a friend. I don't multitask very well, so I forgot one of the most important things to do before pooping. I think the best part was when I realised and told my mate who just freaked out because I was talking to him on the John!}|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_sections_udf = udf(extract_sections, MapType(StringType(), StringType()))\n",
    "annotated_df = df.withColumn(\"AnnotatedSections\", extract_sections_udf(df[\"Prompt\"]))\n",
    "annotated_df.select(\"AnnotatedSections\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d075cd-8f19-40c3-879d-c333c9664067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97246de3-4d96-46b6-932c-ed7c334d5324",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(StringType()))\n",
    "def tokenize_text(text):\n",
    "    tokens = ['title', ':']\n",
    "    tokens += tokenizer.tokenize(text['title'])\n",
    "    tokens += ['text', ':'] + tokenizer.tokenize(text['text'])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39032cde-c891-466a-a1a7-5f1e6b625984",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = annotated_df.withColumn(\"tokens\", tokenize_text(col(\"AnnotatedSections\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36800497-b208-410a-8a19-107e73c708c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Prompt                                                                                                                                                                                                                                                                                                                                                                 |AnnotatedSections                                                                                                                                                                                                                                                                                                                                                      |tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Text: I was on Skype on my tablet as I went to the toilet IMing a friend. I don't multitask very well, so I forgot one of the most important things to do before pooping. I think the best part was when I realised and told my mate who just freaked out because I was talking to him on the John! \\nTitle: forgetting to pull my underwear down before i pooped. ####|{title -> forgetting to pull my underwear down before i pooped., text -> I was on Skype on my tablet as I went to the toilet IMing a friend. I don't multitask very well, so I forgot one of the most important things to do before pooping. I think the best part was when I realised and told my mate who just freaked out because I was talking to him on the John!}|[title, :, ▁forget, ting, ▁to, ▁pull, ▁my, ▁under, we, ar, ▁down, ▁before, ▁i, ▁po, oped, ., text, :, ▁I, ▁was, ▁on, ▁Sk, ype, ▁on, ▁my, ▁table, t, ▁as, ▁I, ▁went, ▁to, ▁the, ▁to, ile, t, ▁IM, ing, ▁a, ▁friend, ., ▁I, ▁don, ', t, ▁mult, it, ask, ▁very, ▁well, ,, ▁so, ▁I, ▁forgot, ▁one, ▁of, ▁the, ▁most, ▁important, ▁things, ▁to, ▁do, ▁before, ▁po, op, ing, ., ▁I, ▁think, ▁the, ▁best, ▁part, ▁was, ▁when, ▁I, ▁real, ised, ▁and, ▁told, ▁my, ▁mate, ▁who, ▁just, ▁fre, aked, ▁out, ▁because, ▁I, ▁was, ▁talking, ▁to, ▁him, ▁on, ▁the, ▁John, !]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "tokenized_df.show(1, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e66dab9-fde7-46fc-8411-17fa6f41b01f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m     \n",
       "\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding_side\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           LlamaTokenizerFast\n",
       "\u001b[0;31mString form:\u001b[0m   \n",
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=10 <...> Token(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "           }\n",
       "           )\n",
       "\u001b[0;31mLength:\u001b[0m         32000\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding.\n",
       "\n",
       "This uses notably ByteFallback and no normalization.\n",
       "\n",
       "```python\n",
       ">>> from transformers import LlamaTokenizerFast\n",
       "\n",
       ">>> tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
       ">>> tokenizer.encode(\"Hello this is a test\")\n",
       "[1, 15043, 445, 338, 263, 1243]\n",
       "```\n",
       "\n",
       "If you want to change the `bos_token` or the `eos_token`, make sure to specify them when initializing the model, or\n",
       "call `tokenizer.update_post_processor()` to make sure that the post-processing is correctly done (otherwise the\n",
       "values of the first token and final token of an encoded sequence will not be correct). For more details, checkout\n",
       "[post-processors] (https://huggingface.co/docs/tokenizers/api/post-processors) documentation.\n",
       "\n",
       "\n",
       "This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
       "refer to this superclass for more information regarding those methods.\n",
       "\n",
       "Args:\n",
       "    vocab_file (`str`, *optional*):\n",
       "        [SentencePiece](https://github.com/google/sentencepiece) file (generally has a .model extension) that\n",
       "        contains the vocabulary necessary to instantiate a tokenizer.\n",
       "    tokenizer_file (`str`, *optional*):\n",
       "        [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that\n",
       "        contains everything needed to load the tokenizer.\n",
       "    clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n",
       "        extra spaces.\n",
       "    unk_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<unk>\"`):\n",
       "        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
       "        token instead.\n",
       "    bos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<s>\"`):\n",
       "        The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n",
       "    eos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"</s>\"`):\n",
       "        The end of sequence token.\n",
       "    add_bos_token (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to add an `bos_token` at the start of sequences.\n",
       "    add_eos_token (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to add an `eos_token` at the end of sequences.\n",
       "    use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not the default system prompt for Llama should be used\n",
       "    legacy (`bool`, *optional*):\n",
       "        Whether or not the `legacy` behavior of the tokenizer should be used. Legacy is before the merge of #24622\n",
       "        and #25224 which includes fixes to properly handle tokens that appear after special tokens.\n",
       "        Make sure to also set `from_slow` to `True`.\n",
       "        A simple example:\n",
       "\n",
       "        - `legacy=True`:\n",
       "        ```python\n",
       "        >>> from transformers import LlamaTokenizerFast\n",
       "\n",
       "        >>> tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\", legacy=True, from_slow=True)\n",
       "        >>> tokenizer.encode(\"Hello <s>.\") # 869 is '▁.'\n",
       "        [1, 15043, 29871, 1, 869]\n",
       "        ```\n",
       "        - `legacy=False`:\n",
       "        ```python\n",
       "        >>> from transformers import LlamaTokenizerFast\n",
       "\n",
       "        >>> tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\", legacy=False, from_slow=True)\n",
       "        >>> tokenizer.encode(\"Hello <s>.\")  # 29889 is '.'\n",
       "        [1, 15043, 29871, 1, 29889]\n",
       "        ```\n",
       "        Checkout the [pull request](https://github.com/huggingface/transformers/pull/24565) for more details.\n",
       "    add_prefix_space (`bool`, *optional*):\n",
       "        Whether or not the tokenizer should automatically add a prefix space\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
       "sequences.\n",
       "\n",
       "Args:\n",
       "    text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
       "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
       "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "    text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
       "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
       "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "    text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
       "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
       "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "    text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
       "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
       "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "\n",
       "    add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
       "        `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
       "        automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n",
       "        automatically.\n",
       "    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
       "        Activates and controls padding. Accepts the following values:\n",
       "\n",
       "        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
       "          sequence if provided).\n",
       "        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
       "          acceptable input length for the model if that argument is not provided.\n",
       "        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
       "          lengths).\n",
       "    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
       "        Activates and controls truncation. Accepts the following values:\n",
       "\n",
       "        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
       "          to the maximum acceptable input length for the model if that argument is not provided. This will\n",
       "          truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
       "          sequences (or a batch of pairs) is provided.\n",
       "        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
       "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
       "          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
       "        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
       "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
       "          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
       "        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
       "          greater than the model maximum admissible input size).\n",
       "    max_length (`int`, *optional*):\n",
       "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
       "\n",
       "        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
       "        is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
       "        length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
       "    stride (`int`, *optional*, defaults to 0):\n",
       "        If set to a number along with `max_length`, the overflowing tokens returned when\n",
       "        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
       "        returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
       "        argument defines the number of overlapping tokens.\n",
       "    is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
       "        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
       "        which it will tokenize. This is useful for NER or token classification.\n",
       "    pad_to_multiple_of (`int`, *optional*):\n",
       "        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
       "        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
       "        `>= 7.5` (Volta).\n",
       "    padding_side (`str`, *optional*):\n",
       "        The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
       "        Default value is picked from the class attribute of the same name.\n",
       "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
       "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
       "\n",
       "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
       "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
       "        - `'np'`: Return Numpy `np.ndarray` objects.\n",
       "\n",
       "    return_token_type_ids (`bool`, *optional*):\n",
       "        Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
       "        the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
       "\n",
       "        [What are token type IDs?](../glossary#token-type-ids)\n",
       "    return_attention_mask (`bool`, *optional*):\n",
       "        Whether to return the attention mask. If left to the default, will return the attention mask according\n",
       "        to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
       "\n",
       "        [What are attention masks?](../glossary#attention-mask)\n",
       "    return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
       "        of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
       "        of returning overflowing tokens.\n",
       "    return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return special tokens mask information.\n",
       "    return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return `(char_start, char_end)` for each token.\n",
       "\n",
       "        This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
       "        Python's tokenizer, this method will raise `NotImplementedError`.\n",
       "    return_length  (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return the lengths of the encoded inputs.\n",
       "    verbose (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to print more information and warnings.\n",
       "    **kwargs: passed to the `self.tokenize()` method\n",
       "\n",
       "Return:\n",
       "    [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
       "\n",
       "    - **input_ids** -- List of token ids to be fed to a model.\n",
       "\n",
       "      [What are input IDs?](../glossary#input-ids)\n",
       "\n",
       "    - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
       "      if *\"token_type_ids\"* is in `self.model_input_names`).\n",
       "\n",
       "      [What are token type IDs?](../glossary#token-type-ids)\n",
       "\n",
       "    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
       "      `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
       "\n",
       "      [What are attention masks?](../glossary#attention-mask)\n",
       "\n",
       "    - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
       "      `return_overflowing_tokens=True`).\n",
       "    - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
       "      `return_overflowing_tokens=True`).\n",
       "    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
       "      regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
       "    - **length** -- The length of the inputs (when `return_length=True`)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16c5f4e5-a2af-443c-b5f6-ce11a2449ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_ids_attention_mask(tokens):\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "    inputs = tokenizer(tokens, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "    token_ids = inputs['input_ids'].tolist()[0] \n",
    "    attention_mask = inputs['attention_mask'].tolist()[0]\n",
    "    \n",
    "    return (token_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5d68e20-b354-4643-82c5-feb093c008d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    " StructField(\"token_ids\", ArrayType(IntegerType()), False),\n",
    " StructField(\"attention_mask\", ArrayType(IntegerType()), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a226583-0914-44fe-9d60-1dbb26dfd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_ids_attention_mask_udf = udf(tokens_to_ids_attention_mask, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c38dacec-02ba-4135-9fce-7b35b621914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_ids_masks = tokenized_df.withColumn(\"ids_masks\", tokens_to_ids_attention_mask_udf(tokenized_df[\"tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00673d09-d27e-49bd-a22a-5c702135ca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |token_ids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |attention_mask                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[title, :, ▁forget, ting, ▁to, ▁pull, ▁my, ▁under, we, ar, ▁down, ▁before, ▁i, ▁po, oped, ., text, :, ▁I, ▁was, ▁on, ▁Sk, ype, ▁on, ▁my, ▁table, t, ▁as, ▁I, ▁went, ▁to, ▁the, ▁to, ile, t, ▁IM, ing, ▁a, ▁friend, ., ▁I, ▁don, ', t, ▁mult, it, ask, ▁very, ▁well, ,, ▁so, ▁I, ▁forgot, ▁one, ▁of, ▁the, ▁most, ▁important, ▁things, ▁to, ▁do, ▁before, ▁po, op, ing, ., ▁I, ▁think, ▁the, ▁best, ▁part, ▁was, ▁when, ▁I, ▁real, ised, ▁and, ▁told, ▁my, ▁mate, ▁who, ▁just, ▁fre, aked, ▁out, ▁because, ▁I, ▁was, ▁talking, ▁to, ▁him, ▁on, ▁the, ▁John, !]|[1, 3611, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000]|[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_final = df_with_ids_masks.select(\"tokens\", \"ids_masks.token_ids\", \"ids_masks.attention_mask\")\n",
    "df_final.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5d3a1-e504-4130-847b-f91da785c12c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
