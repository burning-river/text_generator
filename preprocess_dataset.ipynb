{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2def80c-29f2-4d4d-88b9-cae67a34d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import json\n",
    "import s3fs\n",
    "from io import BytesIO\n",
    "from pyspark.sql import SparkSession\n",
    "from time import time\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, udf\n",
    "import re\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, concat, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc601fd4-9a0d-4300-9a0e-6ea2de590dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "s3_bucket = '**'\n",
    "AWS_ACCESS_KEY_ID = '**'\n",
    "AWS_SECRET_ACCESS_KEY = '**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7701d702-4fa5-4c35-b21a-92745073bca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"subreddits_train_data_2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2a8aa1-12fd-4ce2-a9e3-38503af5cfa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_json_from_s3(s3_bucket, filename, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY):\n",
    "    '''\n",
    "    read json file from S3 bucket\n",
    "    '''\n",
    "    s3 = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                          aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "    obj = s3.get_object(Bucket=s3_bucket, Key=filename)\n",
    "    df = pd.read_json(BytesIO(obj['Body'].read()))\n",
    "\n",
    "    return df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946311b0-2da0-4a32-930a-99ecc8d386f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_s3_as_parquet(df, bucket_name, object_name, access_key_id, secret_access_key):\n",
    "    '''\n",
    "    Compress file to parquet file\n",
    "    '''\n",
    "    s3_resource = boto3.resource('s3', aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)\n",
    "    parquet_buffer = BytesIO()\n",
    "    df.to_parquet(parquet_buffer, index=False, compression='snappy')\n",
    "    s3_resource.Object(bucket_name, object_name).put(Body=parquet_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f5161f-7c15-4a18-bcc7-9352cddb5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_data(parquet_filename):\n",
    "    '''\n",
    "    Save parquet file in S3\n",
    "    '''\n",
    "    df = read_json_from_s3(s3_bucket, filename, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "    save_to_s3_as_parquet(df, s3_bucket, parquet_filename, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd677a9-650f-4bf3-9be4-0bbc610875dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ab61d924-22c8-4ed1-b80e-7196d1b101d6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 390ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ab61d924-22c8-4ed1-b80e-7196d1b101d6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/14ms)\n",
      "25/05/09 17:19:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/09 17:19:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Spark configuration\n",
    "conf = SparkConf()\n",
    "\n",
    "conf.setAll([\n",
    "    (\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\"),\n",
    "    (\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID),\n",
    "    (\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .appName(\"read-s3-with-spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c505ef-58ac-4951-b4c8-28f3019da3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Another home on the eastern plains of Colorado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Colorado stopped using state Medicaid funds on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here’s one of many that are now abandoned-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More Blossoms &amp; Bandos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This house has a very interesting history.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Another home on the eastern plains of Colorado...\n",
       "1  Colorado stopped using state Medicaid funds on...\n",
       "2         Here’s one of many that are now abandoned-\n",
       "3                            More Blossoms & Bandos.\n",
       "4         This house has a very interesting history."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read raw data as pandas dataframe\n",
    "df = read_json_from_s3(s3_bucket, filename, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139bec44-d1e0-487d-8541-91fac6f5f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee2f2f0-ea99-4ddd-9eac-3e3c024cc5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "object_name = 'subreddits_train_data_2.snappy.parquet'\n",
    "save_to_s3_as_parquet(df, s3_bucket, object_name, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2708554-3fc6-45a4-9db6-ff484cc8bce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 17:20:11 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(f\"s3a://{s3_bucket}/subreddits_train_data_2.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e0b8585-52ae-4919-882d-bcc1aa087376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(text='Another home on the eastern plains of Colorado left to time- ')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4f15115-c124-4134-a16b-65accfcf2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df, col):\n",
    "    '''\n",
    "    clean text\n",
    "    '''\n",
    "    # remove https links\n",
    "    # df = df.withColumn(col, regexp_replace(col, \".*<p>\", \"\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, 'http\\S+', \"\"))\n",
    "    # remove punctuation codes\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#33;\", \"!\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#34;\", \"\\\"\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#35;\", \"#\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#37;\", \"%\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#38;\", \"&\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#39;\", \"'\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#40;\", \"(\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#41;\", \")\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#42;\", \"*\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#44;\", \",\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#46;\", \".\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#47;\", \"/\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#58;\", \":\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#59;\", \";\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#63;\", \"?\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"&#64;\", \"@\"))\n",
    "    # remove special characters\n",
    "    df = df.withColumn(col, regexp_replace(col, \"\\*\", \"\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"\\n+\", \" \"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"\\/\", \"or\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"\\'\", \"'\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"\\\"\", \"\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"”\", \"\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"“\", \"\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"\\(.+\\)\", \"\"))\n",
    "    df = df.withColumn(col, regexp_replace(col, \"\\[.+\\]\", \"\"))\n",
    "\n",
    "    df = df.withColumn(col, regexp_replace(col, \" +\", \" \"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca3eb2c2-b95a-44b0-8cf2-29ccebdd0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = clean_text(df, 'text')\n",
    "df_cleaned = df_cleaned.filter(~col(\"text\").startswith(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cfeba61-a6ec-45a7-8d6e-48953971f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(text='Another home on the eastern plains of Colorado left to time- ')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.take(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69763544-c599-4730-af1e-5800563dd672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summary(df):\n",
    "    '''\n",
    "    prepare text for LLM\n",
    "    '''\n",
    "    df = df.withColumn(\"PostText\",\n",
    "        concat(\n",
    "            lit(\"Text: \"), col(\"text\"))\n",
    "                      )\n",
    "    \n",
    "    return df.select(\"PostText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4393f63e-3cc4-42c4-9a7c-2947ff18f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm = text_summary(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccfafd6f-7eac-4f11-9eed-e5639312e042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(PostText='Text: Another home on the eastern plains of Colorado left to time- ')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_llm.take(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4580ef95-a015-44fb-acc4-f52b0897453a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "save_to_s3_as_parquet(df_llm.toPandas(), s3_bucket, 'subreddits_train_data_2.parquet', AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9d2e213-1b6c-48de-bf95-137bbc85ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "85762"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_llm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fffdd4-a8e2-4558-8020-f214ce0bc071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
