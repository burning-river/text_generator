{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3f6612-5fa7-4189-a4e7-612df5321a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import udf, col, current_timestamp\n",
    "from pyspark.sql.types import MapType, StringType, ArrayType, IntegerType, StructType, StructField\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e55d8e-13d2-4e00-95c5-0a711b3b8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 'reddit-tifu'\n",
    "AWS_ACCESS_KEY_ID = '**'\n",
    "AWS_SECRET_ACCESS_KEY = '**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36dd24c3-a730-41b5-8df4-feca142b16c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark(appname, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY):\n",
    "    conf = SparkConf()\n",
    "    \n",
    "    conf.setAll([\n",
    "        (\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\"),\n",
    "        (\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID),\n",
    "        (\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
    "    ])\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName(appname) \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca29721-9077-48dd-8489-d21fbf7dc45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-749d7950-f52e-4498-a0ae-5fa9cbd4125d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 228ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-749d7950-f52e-4498-a0ae-5fa9cbd4125d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "25/05/01 19:20:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark(\"data prep\", AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1857df77-7c8a-4009-b0cd-0b6c47a9b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 19:20:26 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(f\"s3a://{s3_bucket}/subreddits_train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e755cb1-98d5-4615-877f-86ba1752e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_text(text):\n",
    "    keywords = []\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    \n",
    "    for item in nlp(text):\n",
    "        if not item.is_stop:\n",
    "            keywords.append(item.text)\n",
    "    keywords_para = ' '.join(keywords)\n",
    "\n",
    "    return keywords_para\n",
    "\n",
    "def extract_keywords(text):\n",
    "    try:\n",
    "        split_text = text.split(\"Text: \")\n",
    "        text = split_text[1]\n",
    "        keywords_para = shuffle_text(text)\n",
    "\n",
    "        return {\n",
    "        \"text\": text,\n",
    "        \"keywords\": keywords_para\n",
    "        }\n",
    "    except:\n",
    "        return {'text': '', 'keywords': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d59ff0-c250-42bb-a2c1-03109ea740fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee59cbb8-c530-4045-a520-282a95b2c5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(AnnotatedSections={'keywords': 'read questions answers posted , thread invite share like highlight week - interesting discussion , informative answer , insightful question overlooked , .', 'text': \"Nobody can read all the questions and answers that are posted here, so in this thread we invite you to share anything you'd like to highlight from the last week - an interesting discussion, an informative answer, an insightful question that was overlooked, or anything else.\"})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keywords_udf = udf(extract_keywords, MapType(StringType(), StringType()))\n",
    "annotated_df = df_small.withColumn(\"AnnotatedSections\", extract_keywords_udf(df_small[\"PostText\"]))\n",
    "annotated_df.select(\"AnnotatedSections\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3d075cd-8f19-40c3-879d-c333c9664067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c5f4e5-a2af-443c-b5f6-ce11a2449ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_ids_attention_mask(prompt):\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    token_ids_dict = {}\n",
    "    attention_mask_dict = {}\n",
    "    \n",
    "    for section in prompt.keys():\n",
    "        sentence = prompt[section]\n",
    "        inputs = tokenizer(sentence, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "        token_ids_dict[section] = inputs['input_ids'].tolist()[0]\n",
    "        attention_mask_dict[section] = inputs['attention_mask'].tolist()[0]\n",
    "\n",
    "    tokens = {}\n",
    "    for section in prompt.keys():\n",
    "        tokens[section] = [tokenizer.decode(token) for token in token_ids_dict[section]]\n",
    "\n",
    "    for section in prompt.keys():\n",
    "        assert len(token_ids_dict[section]) == len(attention_mask_dict[section]) == len(tokens[section]) \n",
    "        \n",
    "    return (token_ids_dict['keywords'], token_ids_dict['text'], \n",
    "            attention_mask_dict['keywords'], attention_mask_dict['text'], \n",
    "            tokens['keywords'], tokens['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d68e20-b354-4643-82c5-feb093c008d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    " StructField(\"keywords_token_ids\", ArrayType(IntegerType()), False),\n",
    " StructField(\"text_token_ids\", ArrayType(IntegerType()), False),\n",
    " StructField(\"keywords_attention_mask\", ArrayType(IntegerType()), False),\n",
    " StructField(\"text_attention_mask\", ArrayType(IntegerType()), False),\n",
    " StructField(\"keywords_tokens\", StringType(), False),\n",
    " StructField(\"text_tokens\", StringType(), False),   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a226583-0914-44fe-9d60-1dbb26dfd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_ids_attention_mask_udf = udf(tokens_to_ids_attention_mask, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c38dacec-02ba-4135-9fce-7b35b621914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_ids_masks = annotated_df.withColumn(\"ids_masks_tokens\", tokens_to_ids_attention_mask_udf(annotated_df[\"AnnotatedSections\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00673d09-d27e-49bd-a22a-5c702135ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_with_ids_masks.select(\"ids_masks_tokens.keywords_token_ids\", \"ids_masks_tokens.text_token_ids\", \n",
    "                                    \"ids_masks_tokens.keywords_attention_mask\", \"ids_masks_tokens.text_attention_mask\",\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01de3ba5-b88a-4415-8a2f-4db2022f7c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------------+--------------------+\n",
      "|  keywords_token_ids|      text_token_ids|keywords_attention_mask| text_attention_mask|\n",
      "+--------------------+--------------------+-----------------------+--------------------+\n",
      "|[608, 746, 4269, ...|[22009, 54, 608, ...|   [1, 1, 1, 1, 1, 1...|[1, 1, 1, 1, 1, 1...|\n",
      "+--------------------+--------------------+-----------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9caca984-e0c7-461b-9c7f-5fedefb9f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = f\"s3a://{s3_bucket}/subreddits_train_data_tokenized.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e450bd69-f456-4e7e-840c-0b56c6b7b1e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_final.limit(10).write.parquet(s3_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5bbe2-2603-44f2-8fbe-1899b1c1736d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
